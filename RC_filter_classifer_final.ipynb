{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TensorFlow application:\n",
    "#Train a Feed Forward Neural Network to classify passive active filter with 4 labels: Low pass, High pass, Band pass, None of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf   #TensorFlow import\n",
    "import pandas as pd\n",
    "import time as time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"TensorFlow version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Specify directories\n",
    "data_dir =\"/tmp/RC_filter/data/\"\n",
    "model_dir =\"/tmp/RC_filter/\"\n",
    "summaries_dir = \"/tmp/RC_filter/logs/1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract data from CSV\n",
    "df1=pd.read_csv(data_dir+\"RC_filter_classify.csv\")\n",
    "col1 = df1[['R1','C1','R2','C2']]\n",
    "col2 = df1[['LP','HP','BP','NA']]\n",
    "col3 = df1[['fL','fH']]\n",
    "#Convert to Numpy array\n",
    "InputX1 = col1.as_matrix()\n",
    "InputY1 = col2.as_matrix()\n",
    "plotX = col3.as_matrix()\n",
    "InputX1.astype(float, copy=False);\n",
    "InputY1.astype(float, copy=False);\n",
    "plotX.astype(float, copy=False);\n",
    "#print(\"Input:\",InputX1)\n",
    "#print(\"Output:\",InputY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot input data\n",
    "plt.plot(plotX[1:26,0:1],plotX[1:26,1:2], 'ro')  #Low Pass filter\n",
    "plt.plot(plotX[59:74,0:1],plotX[59:74,1:2], 'bo') #Band Pass filter\n",
    "plt.axis([-1e1, 3e3, -1e1, 5e3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "X1_min = np.amin(InputX1,0)     #Find minimum and maximum\n",
    "X1_max = np.amax(InputX1,0)   \n",
    "print(\"Mininum values:\",X1_min)  \n",
    "print(\"Maximum values:\",X1_max)\n",
    "Y1_min = np.amin(InputY1)     \n",
    "Y1_max = np.amax(InputY1) \n",
    "InputX1_norm = (InputX1-X1_min)/(X1_max-X1_min)  #Min-max Normalization on input data\n",
    "InputY1_norm = InputY1  #No normalization on output data\n",
    "#InputY1_norm = (InputY1-Y1_min)/(Y1_max-Y1_min)\n",
    "\n",
    "#Reshape input data [sample_size,input_features]\n",
    "Xfeatures = 4 #Number of input features\n",
    "Yfeatures = 4 #Number of output features\n",
    "samples = 145 # Number of samples\n",
    "\n",
    "InputX1_reshape = np.resize(InputX1_norm,(samples,Xfeatures))\n",
    "InputY1_reshape = np.resize(InputY1_norm,(samples,Yfeatures))\n",
    "#print(\"X1 normalized:\",InputX1_reshape)\n",
    "#print(\"Y1 normalized:\",InputY1_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "batch_size = 130\n",
    "InputX1train = InputX1_reshape[0:batch_size,:]\n",
    "InputY1train = InputY1_reshape[0:batch_size,:]\n",
    "#Validation data\n",
    "v_size = samples-batch_size\n",
    "InputX1v = InputX1_reshape[batch_size:batch_size+v_size,:]\n",
    "InputY1v = InputY1_reshape[batch_size:batch_size+v_size,:]\n",
    "#print(InputX1v)\n",
    "#print(InputY1v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Network hyper parametres\n",
    "learning_rate0 = 0.0001\n",
    "training_epochs = 40000\n",
    "display_epoch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    #Input\n",
    "    X = tf.placeholder(tf.float32,shape=(None,Xfeatures),name=\"X\")#[batch size, input_features]\n",
    "    #Output\n",
    "    Y = tf.placeholder(tf.float32,shape=(None,Yfeatures),name=\"Labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Neural network weights and biases\n",
    "L1 = 4 #Number of neurons in 1st layer\n",
    "#Layer1 weights\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.name_scope('Layer_1'):\n",
    "        W_fc1 = tf.Variable(tf.random_uniform([Xfeatures,L1]),name=\"W\") # [input_features,Number of neurons]) \n",
    "        b_fc1 = tf.Variable(tf.random_uniform([L1]),name=\"bias\")\n",
    "        matmul_fc1=tf.matmul(X, W_fc1) + b_fc1  #Weights * Inputs\n",
    "        tf.summary.histogram(\"Layer1_Weights\",W_fc1)\n",
    "        tf.summary.histogram(\"Layer1_biases\",b_fc1)\n",
    "    with tf.name_scope('ReLU'):\n",
    "        h_fc1 = tf.nn.relu(matmul_fc1)   #ReLU activation\n",
    "        #h_fc1=tf.sigmoid(matmul_fc1)     #Sigmoid activation\n",
    "        \n",
    "#Output layer\n",
    "    with tf.name_scope('Output_Layer') as scope:\n",
    "        W_fO= tf.Variable(tf.random_uniform([L1,Yfeatures]),name=\"W\") #  [Number of neurons in preceding layer,output_features]) \n",
    "        b_fO = tf.Variable(tf.random_uniform([Yfeatures]),name=\"bias\")\n",
    "        matmul_fco= tf.matmul(h_fc1, W_fO) + b_fO\n",
    "        output_layer = matmul_fco  #linear activation\n",
    "        tf.summary.histogram(\"Output_Layer_Weights\",W_fO)\n",
    "        tf.summary.histogram(\"Output_Layer_biases\",b_fO)\n",
    "    with tf.name_scope('Softmax') as scope:\n",
    "        output_layer_prob = tf.nn.softmax(output_layer)  #Applying softmax activation to find probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cost function\n",
    "with tf.device('/cpu:0'):\n",
    "    #Loss/cost function\n",
    "    with tf.name_scope('Cross_Entropy'):\n",
    "        soft_cross_entropy = tf.losses.softmax_cross_entropy(Y,output_layer)\n",
    "        tf.summary.scalar('softmax_cross_entropy', soft_cross_entropy)\n",
    "\n",
    "    #Decreasing learning rate\n",
    "    with tf.name_scope('Learning_rate'):\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = learning_rate0\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,10000, 0.96, staircase=True)\n",
    "#Calculate accuracy\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        #accuracy,update_accuracy = tf.metrics.accuracy(class_labels,class_pred)\n",
    "        class_pred = tf.arg_max(output_layer_prob,dimension=1)  #Find predicted class label (class with highest probability)\n",
    "        class_labels =tf.arg_max(Y,dimension=1)               #Find original class label\n",
    "        correct_prediction = tf.equal(class_pred,class_labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar('Prediction_Accuracy', accuracy)\n",
    "        \n",
    "with tf.device('/cpu:0'):\n",
    "    #Training step\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(soft_cross_entropy,global_step=global_step)\n",
    "        #train_step = tf.train.AdamOptimizer(learning_rate).minimize(soft_cross_entropy,global_step=global_step)\n",
    "        train_step = tf.train.AdagradOptimizer(learning_rate).minimize(soft_cross_entropy,global_step=global_step)\n",
    "    \n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "    \n",
    "#Operation to save variables\n",
    "saver = tf.train.Saver()\n",
    "#Path for tensorboard\n",
    "#tensorboard --logdir=/tmp/RC_Filter/logs/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialization and session\n",
    "init = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()\n",
    "tstart = time.time()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n",
    "    train_writer = tf.summary.FileWriter(summaries_dir,sess.graph)  #For writing summaries\n",
    "    sess.run([init,init_local])  #Initializes all variables\n",
    "    print(\"Initial Training loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    for i in range(training_epochs):\n",
    "        summary,_ = sess.run([merged,train_step],feed_dict={X:InputX1train,Y:InputY1train})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i%display_epoch ==0:\n",
    "            print(\"Iteration:\",i)\n",
    "            print(\"Training loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "            print(\"Training accuracy:\",sess.run([accuracy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "            print(\"Validation loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "            print(\"Learning rate:\",sess.run([learning_rate]))   \n",
    "    #Close summary writer\n",
    "    train_writer.close()\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, model_dir+\"RC_classifier.ckpt\")\n",
    "    #/tmp/RC_filter/RC_classifier.ckpt\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    print(\"Final training loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    print(\"Final Training accuracy:\",sess.run([accuracy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    print(\"Final validation loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "    #print(\"Labels:\",sess.run([class_labels],feed_dict={Y:InputY1train}))\n",
    "    #print(\"Prediction:\",sess.run([class_pred],feed_dict={X:InputX1train}))\n",
    "print(\"Time taken (seconds):\", time.time()-tstart,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Recover model and print weights and biases\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model_dir+\"RC_classifier.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Training loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    print(\"Layer1_Weights:\",sess.run([W_fc1]))\n",
    "    print(\"Layer1_biases:\",sess.run([b_fc1]))\n",
    "    print(\"Output_Layer_Weights:\",sess.run([W_fO]))\n",
    "    print(\"Output_Layer_biases:\",sess.run([b_fO]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "R1 = input(\"Enter Filter Resistance R1 (0.5Kto 10K):\")\n",
    "C1 = input(\"Enter Filter Capacitance C1 (1e-7 to 1e-9):\")\n",
    "R2 = input(\"Enter Filter Resistance R2 (0.5K to 10K):\")\n",
    "C2 = input(\"Enter Filter Capacitance C2 (1e-7 to 1e-9):\")\n",
    "\n",
    "InputX2 = np.asarray([[R1,C1,R2,C2]],dtype=np.float32)\n",
    "InputX2_norm = (InputX2-X1_min)/(X1_max-X1_min)\n",
    "InputX1test = np.resize(InputX2_norm,(1,Xfeatures))\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk for validation.\n",
    "    saver.restore(sess,  model_dir+\"RC_classifier.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    #print(\"Final validation loss:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "    print(\"Filter Type:\",sess.run([output_layer_pred],feed_dict={X:InputX1test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recover model and re-run training session\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(\"summaries_dir\",sess.graph)  #For writing summaries\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, model_dir+\"RC_classifier.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    print(\"Training loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    for i in range(training_epochs):\n",
    "        summary,_ = sess.run([merged,train_step],feed_dict={X:InputX1train,Y:InputY1train})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i%display_epoch ==0:\n",
    "            print(\"Iteration:\",i)\n",
    "            print(\"Training loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "            print(\"Training accuracy:\",sess.run([accuracy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "            print(\"Validation loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "            print(\"Learning rate:\",sess.run([learning_rate]))   \n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, model_dir+\"RC_classifier.ckpt\")\n",
    "    #/tmp/RC_filter/RC_classifier.ckpt\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    print(\"Final training loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    print(\"Final Training accuracy:\",sess.run([accuracy],feed_dict={X:InputX1train,Y:InputY1train}))\n",
    "    print(\"Final validation loss:\",sess.run([soft_cross_entropy],feed_dict={X:InputX1v,Y:InputY1v}))\n",
    "    print(\"Labels:\",sess.run([class_labels],feed_dict={Y:InputY1train}))\n",
    "    print(\"Prediction:\",sess.run([class_pred],feed_dict={X:InputX1train}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
